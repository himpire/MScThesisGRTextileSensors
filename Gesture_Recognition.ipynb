{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▀█▀ ▀▄▀ ▀█▀    ▀█▀ ▄▀▄    ██▀ ▀▄▀ ▄▀▀ ██▀ █   \n",
    " █  █ █  █      █  ▀▄▀    █▄▄ █ █ ▀▄▄ █▄▄ █▄▄ \n",
    "**Text to Excel + Time**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the output directory path\n",
    "output_directory = \"Raw/\"\n",
    "\n",
    "# Step 1: Read text files and extract data\n",
    "for file_number in range(1, 60):\n",
    "    filename = f\"{file_number:03d}.txt\"\n",
    "    with open(f\"Raw/{filename}\") as inf:\n",
    "        lines = inf.readlines()[2:]  # Skip the first two lines\n",
    "        data = [line.strip().split() for line in lines]\n",
    "\n",
    "    # Step 2: Create a DataFrame\n",
    "    columns = [\"1-Thumb\", \"2-Index\", \"3-Middle\", \"4-Ring\", \"5-Pinky\", \"Column6\", \"Column7\", \"Column8\", \"Column9\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Step 3: Convert data in columns to numeric (if possible)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Step 4: Remove the last 4 columns and 2 last rows\n",
    "    df = df.iloc[:, :-4]\n",
    "\n",
    "\n",
    "\n",
    "    # Step 5: Calculate duration (in seconds)\n",
    "    num_samples = len(df)\n",
    "    sampling_rate = 42\n",
    "    duration_s = num_samples / sampling_rate\n",
    "\n",
    "    # Convert duration to milliseconds\n",
    "    duration_ms = duration_s * 1000\n",
    "\n",
    "    # Calculate time increment (ms)\n",
    "    time_increment_ms = 1000 / sampling_rate\n",
    "\n",
    "    # Initialize the \"Time (ms)\" column with zeros\n",
    "    df[\"Time (ms)\"] = 0\n",
    "\n",
    "    # Update the \"Time (ms)\" column for each row\n",
    "    for i in range(num_samples):\n",
    "        df.loc[i, \"Time (ms)\"] = int(i * time_increment_ms)  # Explicitly cast to int\n",
    "    df=df.iloc[370:-100]\n",
    "    \n",
    "    # Save to an Excel file with the same name\n",
    "    output_filename = os.path.join(output_directory, os.path.splitext(filename)[0] + \".xlsx\")\n",
    "    df.to_excel(output_filename, sheet_name=\"Sheet1\", index=False)\n",
    "    print(f\"Saved {output_filename}\")\n",
    "\n",
    "print(\"Conversion completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "█▀▄ ▄▀▄ ▀█▀ ▄▀▄    █▄ ▄█ ▄▀▄ █▄ ▄█ ██▀ █▄ █ ▀█▀ \n",
    "█▄▀ █▀█  █  █▀█    █ ▀ █ ▀▄▀ █ ▀ █ █▄▄ █ ▀█  █  \n",
    "**Adding Data Moments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import plotly.express as px\n",
    "import statistics as sts\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import preprocessing_module as prep\n",
    "columns = ['1-Thumb', '2-Index', '3-Middle', '4-Ring', '5-Pinky']\n",
    "columns_exist = ['1-Thumb', '2-Index', '3-Middle', '4-Ring', '5-Pinky']\n",
    "columns_mean=[]\n",
    "columns_std=[]\n",
    "columns_var=[]\n",
    "columns_der1=[]\n",
    "columns_skew=[]\n",
    "columns_ewm=[]\n",
    "num_parts=7\n",
    "delay=210\n",
    "# Specify the output directory path\n",
    "output_directory = \"DataMoments/\"\n",
    "input_directory = \"Raw/\"\n",
    "rin=1\n",
    "rout=60\n",
    "for column in columns_exist:\n",
    "    mean_column_name = f\"{column}_mean\"\n",
    "    std_column_name = f\"{column}_std\"        \n",
    "    ewm_column_name = f\"{column}_ewm\"\n",
    "    var_column_name = f\"{column}_var\"\n",
    "    Skew_column_name = f\"{column}_skew\"\n",
    "    der1_column_name = f\"{column}_der1\"    \n",
    "    columns_std.append(std_column_name)\n",
    "    columns_der1.append(der1_column_name)        \n",
    "    columns_mean.append(mean_column_name)\n",
    "    columns_var.append(var_column_name)\n",
    "    columns_skew.append(Skew_column_name)\n",
    "    columns_ewm.append(ewm_column_name)\n",
    "# Step 1: Read text files and extract data\n",
    "for file_number in range(rin,rout):\n",
    "    filename = f\"{file_number:03d}.xlsx\"\n",
    "    filepath = os.path.join(input_directory, filename)\n",
    "\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    df2=pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    df3=pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    samplingrate= len(df)/(df['Time (ms)'].iloc[-1]/1000)\n",
    "    part_size = len(df)//num_parts\n",
    "    df=df.iloc[370:]\n",
    "    x_axis = df['Time (ms)'] / 1000\n",
    "    df[\"Label\"]=\"\"\n",
    "\n",
    "    \n",
    "\n",
    "# __________________Mean\n",
    "    for column in columns_exist:\n",
    "        parts = [df2.loc[i * part_size+delay: (i + 1) * part_size, column] for i in range (num_parts)]\n",
    "        for i, part in enumerate(parts):\n",
    "            part_series = pd.Series(part)\n",
    "            mean= sts.mean(part_series)\n",
    "            df2.loc[i * part_size: (i + 1) * part_size, column]= mean\n",
    "            df[f\"{column}_mean\"]=df2[column]\n",
    "            \n",
    "# █ █ ▄▀▄ █▀▄ █ ▄▀▄ █▄ █ ▄▀▀ ██▀ \n",
    "# ▀▄▀ █▀█ █▀▄ █ █▀█ █ ▀█ ▀▄▄ █▄▄ \n",
    "# _____________________Varinace\n",
    "    for column in columns_exist:\n",
    "        df[f\"{column}_var\"] = df[column].rolling(window=3).var()\n",
    "        df[f\"{column}_var\"].bfill()\n",
    "# _____________________\n",
    "# ▄▀▀ ▀█▀ █▀▄ \n",
    "# ▄█▀  █  █▄▀ \n",
    "    # for column in columns_exist:\n",
    "    #     df[f\"{column}_std\"] = df[column].rolling(window=2).std()\n",
    "    #     df[f\"{column}_std\"].bfill()\n",
    "\n",
    "# ▄▀▀ █▄▀ ██▀ █   █ \n",
    "# ▄█▀ █ █ █▄▄ ▀▄▀▄▀ \n",
    "# _____________________Skew\n",
    "    for column in columns_exist:\n",
    "        df[f\"{column}_skew\"] = abs(df2[column].rolling(window=3).skew().shift(-110))\n",
    "        df[f\"{column}_skew\"].bfill()\n",
    "\n",
    "\n",
    "    # fig = go.Figure()\n",
    "    # fig2= go.Figure()\n",
    "    # fig3 = go.Figure()\n",
    "    # for column in columns:\n",
    "    #     # fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_std\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_skew\"]*400, mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}\"], mode='lines', name=f'{column}'))\n",
    "    # fig.show()\n",
    "\n",
    "    # fig.update_layout(\n",
    "    #     title='1st derivative',\n",
    "    #     xaxis_title='time',\n",
    "    #     yaxis_title='Elec Res',\n",
    "    #     legend_title='Legends'\n",
    "    # )\n",
    "\n",
    "    output_filename = os.path.join(output_directory, os.path.splitext(filename)[0] + \".xlsx\")\n",
    "    df.to_excel(output_filename, sheet_name=\"Sheet1\", index=False)\n",
    "    print(f\"Saved {output_filename}\")\n",
    "\n",
    "print(\"Conversion completed successfully!\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▄▀▀ ▄▀▄ ▄▀▀ █   ██▀ \n",
    "▄█▀ █▀█ ▀▄▄ █▄▄ █▄▄ \n",
    "**Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import plotly.express as px\n",
    "import statistics as sts\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import preprocessing_module as prep\n",
    "columns = ['1-Thumb', '2-Index', '3-Middle', '4-Ring', '5-Pinky']\n",
    "columns_exist = ['1-Thumb', '2-Index', '3-Middle', '4-Ring', '5-Pinky']\n",
    "columns_mean=[]\n",
    "columns_std=[]\n",
    "columns_var=[]\n",
    "columns_der1=[]\n",
    "columns_skew=[]\n",
    "columns_MinMax=[]\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "rin = 1\n",
    "rout = 60\n",
    "\n",
    "MeanNormalization_path = \"Scaled/Mean Normalization\"\n",
    "MinMaxScaler_path = \"Scaled/MinMax scaler\"\n",
    "robustscaler_path = \"Scaled/robustscaler\"\n",
    "standarization_path = \"Scaled/standardized\"\n",
    "num_parts = 7\n",
    "delay = 210\n",
    "# Specify the output directory path\n",
    "output_directory = \"Scaled/\"\n",
    "input_directory = \"DataMoments/\"\n",
    "for column in columns_exist:\n",
    "    mean_column_name = f\"{column}_mean\"\n",
    "    std_column_name = f\"{column}_std\"        \n",
    "    var_column_name = f\"{column}_var\"\n",
    "    Skew_column_name = f\"{column}_skew\"\n",
    "    MinMax_column_name = f\"{column}_MinMax\"    \n",
    "    columns_std.append(std_column_name)\n",
    "    columns_mean.append(mean_column_name)\n",
    "    columns_var.append(var_column_name)\n",
    "    columns_skew.append(Skew_column_name)\n",
    "    columns_MinMax.append(MinMax_column_name)\n",
    "\n",
    "\n",
    "\n",
    "for file_number in range(rin, rout):\n",
    "    filename = f\"{file_number:03d}.xlsx\"\n",
    "    filepath = os.path.join(input_directory, filename)\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    df2 = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    df3 = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    df4 = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    part_size = len(df)//num_parts\n",
    "    # Step 2: Create a DataFrame\n",
    "    x_axis = df['Time (ms)'] / 1000\n",
    "\n",
    "\n",
    "# \n",
    "# █▄ ▄█ █ █▄ █    █▄ ▄█ ▄▀▄ ▀▄▀ \n",
    "# █ ▀ █ █ █ ▀█    █ ▀ █ █▀█ █ █ \n",
    "    for column in columns:\n",
    "        df[f\"{column}_MinMax\"]=prep.min_max_scaling(df[column])\n",
    "    fig=go.Figure()\n",
    "    # for column in columns:\n",
    "    #     # fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_std\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_skew\"]/max(df[f\"{column}_skew\"]), mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_MinMax\"], mode='lines', name=f'{column}'))\n",
    "    # fig.show()\n",
    "    \n",
    "# ▄▀▀ ▀█▀ ▄▀▄ █▄ █ █▀▄ ▄▀▄ █▀▄ █▀▄ █ ▀█▀ ██▀ █▀▄ \n",
    "# ▄█▀  █  █▀█ █ ▀█ █▄▀ █▀█ █▀▄ █▄▀ █ █▄▄ █▄▄ █▄▀ \n",
    "    for column in columns:\n",
    "        df[f\"{column}_Standardized\"]=prep.mean_normalization(df[column])\n",
    "    # fig=go.Figure()\n",
    "    # for column in columns:\n",
    "        # fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_std\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_skew\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_Standardized\"], mode='lines', name=f'{column}'))\n",
    "    # fig.show()\n",
    "    \n",
    "# █▀▄ ▄▀▄ ██▄ █ █ ▄▀▀ ▀█▀ \n",
    "# █▀▄ ▀▄▀ █▄█ ▀▄█ ▄█▀  █  \n",
    "    for column in columns:\n",
    "        df[f\"{column}_RobustScaler\"]=prep.robust_scaler(df[column])\n",
    "    # fig=go.Figure()\n",
    "    # for column in columns:\n",
    "    #     # fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_std\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_skew\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_RobustScaler\"], mode='lines', name=f'{column}'))\n",
    "    # fig.show()\n",
    "    \n",
    "    # for column in columns:\n",
    "    #     asd=prep.calculate_section_func(df,f\"{column}_mean\",0,0,\"skew\",prep.first_derivative,column)\n",
    "    #     print(asd)\n",
    "    # fig=go.Figure()\n",
    "    # for column in columns:\n",
    "    #     # fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_std\"], mode='lines', name=f'{column}'))\n",
    "    #     # fig.add_trace(go.Scatter(x=df.index, y=df[f\"{column}_skew\"], mode='lines', name=f'{column}'))\n",
    "    #     fig.add_trace(go.Scatter(x=x_axis, y=asd, mode='lines', name=f'{column}'))\n",
    "    # fig.show()    \n",
    "    # fig = px.scatter(df, x=x_axis, y=columns_to_plot, title=f\" Mean segmented   {filename} basedon    Time (ms)\")\n",
    "    # fig.update_xaxes(title_text=\"Time (s)\")\n",
    "    # fig.update_yaxes(title_text=\"Electrical Resistance (ohms)\")\n",
    "    # fig.show()\n",
    "    # fig = px.line(df, x=x_axis, y=columns_Norm,\n",
    "    #             title=f\" MinMaxNorm {filename}    Time (ms) \")\n",
    "    # fig.update_xaxes(title_text=\"Time (s)\")\n",
    "    # fig.update_yaxes(title_text=\"Electrical Resistance (ohms)\")\n",
    "    # fig.show()\n",
    "    output_filename = os.path.join(output_directory, os.path.splitext(filename)[0] + \"_Sacled.xlsx\")\n",
    "    df.to_excel(output_filename, sheet_name=\"Sheet1\", index=False)\n",
    "    print(f\"Saved {output_filename}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "█▀ ██▀ ▄▀▄ ▀█▀ █ █ █▀▄ ██▀ ▄▀▀ \n",
    "█▀ █▄▄ █▀█  █  ▀▄█ █▀▄ █▄▄ ▄█▀ \n",
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import plotly.express as px\n",
    "import statistics as sts\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import preprocessing_module as prep\n",
    "\n",
    "\n",
    "columns = ['1-Thumb', '2-Index', '3-Middle', '4-Ring', '5-Pinky']\n",
    "columns_mean=[]\n",
    "columns_std=[]\n",
    "columns_var=[]\n",
    "columns_der1=[]\n",
    "columns_skew=[]\n",
    "columns_MinMax=[]\n",
    "columns_Standardized=[]\n",
    "columns_RobustScaler=[]\n",
    "columns_closure =[]\n",
    "input_directory = \"Scaled/\"\n",
    "output_directory = \"Features/\"\n",
    "rin=1\n",
    "rout=60\n",
    "num_parts=7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "Gesturs_Label=['g1', 'g2','g3','g4','g5','g6','g7']\n",
    "Gestures_Closures=[\n",
    "    [0,1,0,0,0,0,1],\n",
    "    [0,1,1,1,1,1,1],\n",
    "    [0,1,0,1,1,1,0],\n",
    "    [0,1,0,0,1,1,0],\n",
    "    [0,1,0,0,0,1,1]\n",
    "] \n",
    "\n",
    "columns_Features = [\n",
    "    \"1-Thumb-closure\", \"2-Index-closure\", \"3-Middle-closure\", \"4-Ring-closure\", \"5-Pinky-closure\",\n",
    "    \"1-Thumb-mean\", \"2-Index-mean\", \"3-Middle-mean\", \"4-Ring-mean\", \"5-Pinky-mean\",\n",
    "    \"1-Thumb-median\", \"2-Index-median\", \"3-Middle-median\", \"4-Ring-median\", \"5-Pinky-median\",\n",
    "    \"1-Thumb-max\", \"2-Index-max\", \"3-Middle-max\", \"4-Ring-max\", \"5-Pinky-max\",\n",
    "    \n",
    "    \"Gesture\"  # Column for gesture label\n",
    "]\n",
    "\n",
    "for column in columns:\n",
    "    mean_column_name = f\"{column}_mean\"\n",
    "    columns_mean.append(columns_mean)\n",
    "    var_column_name = f\"{column}_var\"\n",
    "    columns_var.append(columns_var)\n",
    "    std_column_name = f\"{column}_std\"\n",
    "    columns_std.append(columns_std)\n",
    "    skew_column_name = f\"{column}_skew\"\n",
    "    columns_skew.append(columns_skew)\n",
    "    MinMax_column_name = f\"{column}_MinMax\"\n",
    "    columns_MinMax.append(columns_MinMax)\n",
    "    Standardized_column_name = f\"{column}_Standardized\"\n",
    "    columns_Standardized.append(columns_Standardized)\n",
    "    RobustScaler_column_name = f\"{column}_RobustScaler\"\n",
    "    columns_RobustScaler.append(columns_RobustScaler)\n",
    "    closure_column_name = f\"{column}_closure\"\n",
    "    columns_closure.append(columns_closure)   \n",
    "\n",
    "    \n",
    "\n",
    "# ---------------------------------------------------------\n",
    "for file_number in range(rin, rout):\n",
    "    filename = f\"{file_number:03d}_Sacled.xlsx\"\n",
    "    filepath = os.path.join(input_directory, filename)\n",
    "    df = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    part_size = len(df)//num_parts    \n",
    "    # Feature Data Frame\n",
    "    # df_features = pd.DataFrame()\n",
    "    df_copy=df.copy()\n",
    "    data_to_save = []\n",
    "    \n",
    "    Means=[]\n",
    "    Medians=[]\n",
    "    Maxs=[]\n",
    "\n",
    "\n",
    "            # █▄ ▄█ ██▀ ▄▀▄ █▄ █    █   █ █ █▄ █ █▀▄ ▄▀▄ █   █ ▄▀▀ \n",
    "            # █ ▀ █ █▄▄ █▀█ █ ▀█    ▀▄▀▄▀ █ █ ▀█ █▄▀ ▀▄▀ ▀▄▀▄▀ ▄█▀     \n",
    "            # calculating means on windows\n",
    "    for col in columns:\n",
    "        indices= prep.find_peaks_indices(1,df,col,\"skew\")\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            means1=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmean)\n",
    "            Means.append(means1)\n",
    "        if col==\"2-Index\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            means2=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmean)\n",
    "            Means.append(means2) \n",
    "        if col==\"3-Middle\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            means3=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmean)\n",
    "            Means.append(means3)    \n",
    "        if col==\"4-Ring\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            means4=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmean)\n",
    "            Means.append(means4)    \n",
    "        if col==\"5-Pinky\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            means5=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmean)\n",
    "            Means.append(means5)\n",
    "\n",
    "\n",
    "    \n",
    "# █▄ ▄█ ██▀ █▀▄ █ ▄▀▄ █▄ █    █   █ █ █▄ █ █▀▄ ▄▀▄ █   █             \n",
    "# █ ▀ █ █▄▄ █▄▀ █ █▀█ █ ▀█    ▀▄▀▄▀ █ █ ▀█ █▄▀ ▀▄▀ ▀▄▀▄▀                 \n",
    "    for col in columns:\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            medians1=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmedian)\n",
    "            Medians.append(medians1)\n",
    "        if col==\"2-Index\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            medians2=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmedian)\n",
    "            Medians.append(medians2) \n",
    "        if col==\"3-Middle\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            medians3=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmedian)\n",
    "            Medians.append(medians3)    \n",
    "        if col==\"4-Ring\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            medians4=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmedian)\n",
    "            Medians.append(medians4)    \n",
    "        if col==\"5-Pinky\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,func=prep.wmean,w=120,move=80,mode=\"skew\")\n",
    "            medians5=prep.calculate_section_sts(df=df_copy,col=f\"{col}_MinMax\",col2=col,height=0,move=-80,mode=\"skew\",func=prep.pmedian)\n",
    "            Medians.append(medians5)    \n",
    "   \n",
    "# █▄ ▄█ █ █▄ █       █▄ ▄█ ▄▀▄ ▀▄▀             \n",
    "# █ ▀ █ █ █ ▀█       █ ▀ █ █▀█ █ █\n",
    "    for col in columns:\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, func=prep.wmean, w=120, move=80, mode=\"skew\")\n",
    "            max1=prep.calculate_section_sts(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, move=-80, mode=\"skew\", func=prep.peak_amplitude)\n",
    "            Maxs.append(max1)\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, func=prep.wmean, w=120, move=80, mode=\"skew\")\n",
    "            max2=prep.calculate_section_sts(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, move=-80, mode=\"skew\", func=prep.peak_amplitude)\n",
    "            Maxs.append(max2)\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, func=prep.wmean, w=120, move=80, mode=\"skew\")\n",
    "            max3=prep.calculate_section_sts(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, move=-80, mode=\"skew\", func=prep.peak_amplitude)\n",
    "            Maxs.append(max3)\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, func=prep.wmean, w=120, move=80, mode=\"skew\")\n",
    "            max4=prep.calculate_section_sts(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, move=-80, mode=\"skew\", func=prep.peak_amplitude)\n",
    "            Maxs.append(max4)\n",
    "        if col==\"1-Thumb\":\n",
    "            for i in range(0):\n",
    "                prep.calculate_sts_sections(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, func=prep.wmean, w=120, move=80, mode=\"skew\")\n",
    "            max5=prep.calculate_section_sts(df=df_copy, col=f\"{col}_MinMax\", col2=col, height=0, move=-80, mode=\"skew\", func=prep.peak_amplitude)\n",
    "            Maxs.append(max5)\n",
    "\n",
    "\n",
    "\n",
    "# ▄▀▀ █▀▄ ██▀ ▄▀▄ ▀█▀ █ █▄ █ ▄▀     █▀ █ █   ██▀ ▄▀▀ \n",
    "# ▀▄▄ █▀▄ █▄▄ █▀█  █  █ █ ▀█ ▀▄█    █▀ █ █▄▄ █▄▄ ▄█▀ \n",
    "    df_features = pd.DataFrame(columns=columns_Features)\n",
    "    \n",
    "    # Add rows of data\n",
    "    for i in range(7):  # 7 rows\n",
    "        row = []\n",
    "\n",
    "        # Append Gesture Closures\n",
    "        row.extend(Gestures_Closures[j][i] for j in range(len(Gestures_Closures)))\n",
    "\n",
    "        # Append Means\n",
    "        row.extend(Means[j][i] for j in range(len(Means)))\n",
    "\n",
    "        # Append Medians\n",
    "        row.extend(Medians[j][i] for j in range(len(Medians)))\n",
    "\n",
    "        # Append Maxs\n",
    "        row.extend(Maxs[j][i] for j in range(len(Maxs)))\n",
    "\n",
    "        # Append the gesture label\n",
    "        row.append(Gesturs_Label[i])  \n",
    "        # print(row)\n",
    "    \n",
    "        df_features.loc[i] = row \n",
    "    # Create a DataFrame from the data\n",
    "    output_filename = os.path.join(output_directory, f\"{file_number:03d}_Featured.xlsx\")  # Unique filename\n",
    "    df_features.to_excel(output_filename, sheet_name=\"Sheet1\", index=False)\n",
    "    print(f\"Saved {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▄▀▀ ▄▀▄ █▄ ▄█ ██▄ █ █▄ █ ██▀ \n",
    "▀▄▄ ▀▄▀ █ ▀ █ █▄█ █ █ ▀█ █▄▄ \n",
    "**Combine all files to one unified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify your input directory\n",
    "input_directory = \"Features/\"\n",
    "output_directory = \"Features/\"\n",
    "output_filename = os.path.join(output_directory, os.path.splitext(\"Features3\")[0] + \".xlsx\")\n",
    "\n",
    "# Initialize an empty DataFrame to hold the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file\n",
    "for file_number in range(1, 60):\n",
    "    filename = f\"{file_number:03d}_Featured.xlsx\"\n",
    "    filepath = os.path.join(input_directory, filename)\n",
    "\n",
    "    df = pd.read_excel(filepath, sheet_name=\"Sheet1\")\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)  # Concatenate data\n",
    "combined_df.iloc[:, 5:21] = combined_df.iloc[:, 5:21].round(3)\n",
    "# Save the combined data to the destination Excel file\n",
    "with pd.ExcelWriter(output_filename, mode=\"w\", engine=\"openpyxl\") as writer:\n",
    "    combined_df.to_excel(writer, sheet_name=\"Combined_Data\", index=False)\n",
    "\n",
    "print(f\"Data saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▄▀▄ █   ▄▀  ▄▀▄ █▀▄ █ ▀█▀ █▄█ █▄ ▄█ ▄▀▀ \n",
    "█▀█ █▄▄ ▀▄█ ▀▄▀ █▀▄ █  █  █ █ █ ▀ █ ▄█▀ \n",
    "**Algorithms**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import catboost\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"  # Replace 4\n",
    "# ... (set LOKY_MAX_CPU_COUNT as shown above)\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "n_folds = 30\n",
    "\n",
    "# Define the list of classifiers to use\n",
    "classifiers = [\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"KNN\", KNeighborsClassifier()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Neural Network\", MLPClassifier(\n",
    "        max_iter=1000,  # Still set a high max_iter for safety\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,  # Portion of data for validation\n",
    "        n_iter_no_change=10,  # Number of epochs without improvement \n",
    "    )),\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"Naive Bayes\", GaussianNB()),\n",
    "    (\"AdaBoost\", AdaBoostClassifier(algorithm='SAMME')),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"Stochastic Gradient Descent\", SGDClassifier()),\n",
    "    # (\"XGBoost\", xgb.XGBClassifier()),  # You'll need to install XGBoost\n",
    "    (\"LightGBM\", lgb.LGBMClassifier()), # You'll need to install LightGBM\n",
    "    (\"CatBoost\", catboost.CatBoostClassifier()) # You'll need to install CatBoost\n",
    "]\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each feature file\n",
    "df = pd.read_excel(\"Features/Features3.xlsx\")\n",
    "    # Extract the features and labels\n",
    "X = df.iloc[:, 6:20]\n",
    "y = df[\"1-Thumb-closure\"]\n",
    "\n",
    "    # Initialize the k-fold cross-validation object\n",
    "kf = StratifiedKFold(n_splits=n_folds, shuffle=True,random_state=43)    # Iterate over each fold\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    # Iterate over each classifier\n",
    "    for name, classifier in classifiers:\n",
    "        # Train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Make predictions on the testing data\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        # Calculate the accuracy score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        # print(f\"Unique labels in y_test: {set(y_test)}\")\n",
    "        # print(f\"Unique labels in y_pred: {set(y_pred)}\")\n",
    "        # Calculate the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        # Store the results\n",
    "        i=1\n",
    "        if (name, i) not in results:\n",
    "            results[(name, i)] = []\n",
    "        results[(name, i)].append((accuracy, cm))\n",
    "\n",
    "# Print the results and plot confusion matrices\n",
    "for (name, i), accuracy_cm_tuples in results.items():\n",
    "    average_accuracy = sum(acc for acc, cm in accuracy_cm_tuples) / len(accuracy_cm_tuples)\n",
    "    print(f\"Classifier: {name}, File: {i}, Average Accuracy: {average_accuracy}\")\n",
    "\n",
    "    # Plot confusion matrices for each fold\n",
    "    for j, (acc, cm) in enumerate(accuracy_cm_tuples):\n",
    "        print(f\"  Fold {j + 1} Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        plt.figure(figsize=(2, 1))  # Adjust figure size as needed\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(f\"Confusion Matrix - {name} - File {i} - Fold {j + 1}\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.show()\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
